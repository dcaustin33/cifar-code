{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb71608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state \n",
    "\n",
    "from utils import get_accuracy\n",
    "from lars import LARSWrapper\n",
    "import cifar_100\n",
    "from logger import log_metrics as logger\n",
    "import torch_trainer as trainer\n",
    "from cifar_resnet import resnet18\n",
    "import cifar_100\n",
    "\n",
    "#python helper inputs\n",
    "import os\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "dataset_args = {\n",
    "                 'crop_size': 32,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .8, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.16, \n",
    "                 'max_scale': 0.9}\n",
    "val_dataset_args = {\n",
    "                 'crop_size': 32,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': 0, \n",
    "                 'gray_scale_prob': 0, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': 0, \n",
    "                 'min_scale': 0.9, \n",
    "                 'max_scale': 1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(dataset_args, val_dataset_args):\n",
    "    \n",
    "    train_dataset = cifar_100.CIFAR_100_transformations(train = True, **dataset_args)\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle = True,\n",
    "        batch_size=256,\n",
    "        num_workers=8,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    val_dataset = cifar_100.CIFAR_100_transformations(views = 1, train = True, **val_dataset_args)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle = True,\n",
    "        batch_size=256,\n",
    "        num_workers=8,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    return train_dataset, dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def create_params(model, example, rng):\n",
    "    model = model\n",
    "    batch = example  # (N, H, W, C) format\n",
    "    params = model.init(rng, batch)['params']\n",
    "    return params\n",
    "\n",
    "\n",
    "def create_optimizer(optimizer, lr, wd):\n",
    "    optimizer = optimizer(lr)\n",
    "    return optimizer\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels).mean()\n",
    "\n",
    "\n",
    "def create_train_state(rng, optimizer):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    model = resnet18()\n",
    "    batch = jnp.ones((4, 32, 32, 3))  # (N, H, W, C) format\n",
    "    params = model.init(jax.random.PRNGKey(0), batch)['params']\n",
    "    tx = optimizer\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "def loss_fn(params, data):\n",
    "    logits, _ = model.apply({'params': params}, data['image0'], mutable=['batchstats'])\n",
    "    loss = jnp.mean(jax.vmap(cross_entropy_loss)(logits=logits, labels=data['label']), axis= 0)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def training_step(state, data):\n",
    "    \n",
    "    def loss_fn(params, data):\n",
    "        logits, _ = model.apply({'params': params}, data['image0'], mutable=['batch_stats'])\n",
    "        loss = jnp.mean(jax.vmap(cross_entropy_loss)(logits=logits, labels=data['label']), axis= 0)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params, data)\n",
    "    #state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "   # metrics['total'] += data['image0'].shape[0]\n",
    "    #metrics['Accuracy'] += acc1\n",
    "    #metrics['Accuracy Top 5'] += acc5\n",
    "\n",
    "    return state#, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494288cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dataloader, val_dataloader = prepare_data(dataset_args, val_dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3c46e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.564973831176758\n",
      "10 3.9388911724090576\n",
      "20 4.651685953140259\n",
      "30 5.6907055377960205\n",
      "40 6.692582845687866\n",
      "50 8.019368171691895\n",
      "60 8.91062617301941\n",
      "70 9.836599349975586\n",
      "80 10.723158359527588\n",
      "90 11.835393190383911\n",
      "100 12.726295232772827\n",
      "110 13.458921670913696\n",
      "120 14.38868522644043\n",
      "130 15.744547367095947\n",
      "140 16.639466524124146\n",
      "150 17.463393211364746\n",
      "160 18.32213807106018\n",
      "170 19.504474878311157\n",
      "180 20.501022338867188\n",
      "190 21.012498378753662\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "optimizer = optax.adamw(.001)\n",
    "state = create_train_state(init_rng, optimizer)\n",
    "del init_rng  # Must not be used anymore.\n",
    "model = resnet18()\n",
    "\n",
    "import time\n",
    "now = time.time()\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    data['image0'] = jnp.array(data['image0'].permute(0, 2, 3, 1))\n",
    "    data['label'] = jnp.array(data['label'])\n",
    "    state = training_step(state, data)\n",
    "    #logits = resnet18().apply({'params': state.params}, data['image0'], mutable=['batch_stats'])\n",
    "    if i % 10 == 0: print(i, time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a0b38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from haiku._src import batch_norm\n",
    "hk.BatchNorm = batch_norm.BatchNorm\n",
    "class MyModuleCustom(hk.Module):\n",
    "  def __init__(self):\n",
    "    \n",
    "    super().__init__()\n",
    "    bn_config = dict()\n",
    "    bn_config.setdefault(\"create_scale\", True)\n",
    "    bn_config.setdefault(\"create_offset\", True)\n",
    "    bn_config.setdefault(\"decay_rate\", 0.999)\n",
    "    self.linear_1 = hk.Linear(100)\n",
    "    self.bn1 = hk.BatchNorm(**bn_config)\n",
    "    self.linear_2 = hk.Linear(1)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.linear_2(self.bn1(self.linear_1(x), True))\n",
    "\n",
    "\n",
    "class MLP(hk.Module):\n",
    "  \"\"\"One hidden layer perceptron, with normalization.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self._hidden_size = 10\n",
    "    self._output_size = 1\n",
    "    bn_config = dict()\n",
    "    bn_config.setdefault(\"create_scale\", True)\n",
    "    bn_config.setdefault(\"create_offset\", True)\n",
    "    bn_config.setdefault(\"decay_rate\", 0.999)\n",
    "    self._bn_config = bn_config\n",
    "\n",
    "  def __call__(self, inputs: jnp.ndarray, is_training: bool) -> jnp.ndarray:\n",
    "    out = hk.Linear(output_size=self._hidden_size, with_bias=True)(inputs)\n",
    "    out = hk.BatchNorm(**self._bn_config)(out, is_training=is_training)\n",
    "    out = jax.nn.relu(out)\n",
    "    out = hk.Linear(output_size=self._output_size, with_bias=False)(out)\n",
    "    return out    \n",
    "\n",
    "def forward(x):\n",
    "  module = MLP()\n",
    "  return module(x, True)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "forward = hk.transform_with_state(forward)\n",
    "params, state = forward.init(init_rng, jnp.ones((4, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "febe9c05",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ec2-user/cifar/cifar-code/Untitled.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/cifar/cifar-code/Untitled.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(logits)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/cifar/cifar-code/Untitled.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ((logits \u001b[39m-\u001b[39m y)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224177735f6563325f736d616c6c227d/home/ec2-user/cifar/cifar-code/Untitled.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m grad_fn \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mgrad(loss_fn)(state, params, x, y)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/jax/_src/api.py:1109\u001b[0m, in \u001b[0;36m_check_input_dtype_revderiv\u001b[0;34m(name, holomorphic, allow_int, x)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m (dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mbool_)):\n\u001b[1;32m   1108\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_int:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m requires real- or complex-valued inputs (input dtype \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1110\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat is a sub-dtype of np.inexact), but got \u001b[39m\u001b[39m{\u001b[39;00maval\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1111\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mIf you want to use Boolean- or integer-valued inputs, use vjp \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1112\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mor set allow_int to True.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minexact):\n\u001b[1;32m   1114\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m requires numerical-valued inputs (input dtype that is a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1115\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msub-dtype of np.bool_ or np.number), but got \u001b[39m\u001b[39m{\u001b[39;00maval\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True."
     ]
    }
   ],
   "source": [
    "x = jax.random.normal(init_rng, (4, 3))\n",
    "y = jnp.array([1.0, 2.1, 1.4, 1.2])\n",
    "def loss_fn(state, params, x, y):\n",
    "    logits = forward.apply(params, state, None, x)\n",
    "    print(logits)\n",
    "    return ((logits - y)**2).mean()\n",
    "grad_fn = jax.grad(loss_fn)(state, params, x, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
