{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb71608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state \n",
    "\n",
    "from utils import get_accuracy\n",
    "from lars import LARSWrapper\n",
    "import cifar_100\n",
    "from logger import log_metrics as logger\n",
    "import torch_trainer as trainer\n",
    "from cifar_resnet import resnet18\n",
    "import cifar_100\n",
    "\n",
    "#python helper inputs\n",
    "import os\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "dataset_args = {\n",
    "                 'crop_size': 32,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': .8, \n",
    "                 'gray_scale_prob': 0.2, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': .5, \n",
    "                 'min_scale': 0.16, \n",
    "                 'max_scale': 0.9}\n",
    "val_dataset_args = {\n",
    "                 'crop_size': 32,\n",
    "                 'brightness': 0.4, \n",
    "                 'contrast': 0.4, \n",
    "                 'saturation': .2, \n",
    "                 'hue': .1, \n",
    "                 'color_jitter_prob': 0, \n",
    "                 'gray_scale_prob': 0, \n",
    "                 'horizontal_flip_prob': 0.5, \n",
    "                 'gaussian_prob': 0, \n",
    "                 'min_scale': 0.9, \n",
    "                 'max_scale': 1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(dataset_args, val_dataset_args):\n",
    "    \n",
    "    train_dataset = cifar_100.CIFAR_100_transformations(train = True, **dataset_args)\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle = True,\n",
    "        batch_size=256,\n",
    "        num_workers=8,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    val_dataset = cifar_100.CIFAR_100_transformations(views = 1, train = True, **val_dataset_args)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle = True,\n",
    "        batch_size=256,\n",
    "        num_workers=8,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    return train_dataset, dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def create_params(model, example, rng):\n",
    "    model = model\n",
    "    batch = example  # (N, H, W, C) format\n",
    "    params = model.init(rng, batch)['params']\n",
    "    return params\n",
    "\n",
    "\n",
    "def create_optimizer(optimizer, lr, wd):\n",
    "    optimizer = optimizer(lr)\n",
    "    return optimizer\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels).mean()\n",
    "\n",
    "\n",
    "def create_train_state(rng, optimizer):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    model = resnet18()\n",
    "    batch = jnp.ones((4, 32, 32, 3))  # (N, H, W, C) format\n",
    "    params = model.init(jax.random.PRNGKey(0), batch)['params']\n",
    "    tx = optimizer\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "def loss_fn(params, data):\n",
    "    logits, _ = model.apply({'params': params}, data['image0'], mutable=['batchstats'])\n",
    "    loss = jnp.mean(jax.vmap(cross_entropy_loss)(logits=logits, labels=data['label']), axis= 0)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def training_step(state, data):\n",
    "    \n",
    "    def loss_fn(params, data):\n",
    "        logits, _ = model.apply({'params': params}, data['image0'], mutable=['batch_stats'])\n",
    "        loss = jnp.mean(jax.vmap(cross_entropy_loss)(logits=logits, labels=data['label']), axis= 0)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params, data)\n",
    "    #state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "   # metrics['total'] += data['image0'].shape[0]\n",
    "    #metrics['Accuracy'] += acc1\n",
    "    #metrics['Accuracy Top 5'] += acc5\n",
    "\n",
    "    return state#, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494288cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dataloader, val_dataloader = prepare_data(dataset_args, val_dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3c46e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.564973831176758\n",
      "10 3.9388911724090576\n",
      "20 4.651685953140259\n",
      "30 5.6907055377960205\n",
      "40 6.692582845687866\n",
      "50 8.019368171691895\n",
      "60 8.91062617301941\n",
      "70 9.836599349975586\n",
      "80 10.723158359527588\n",
      "90 11.835393190383911\n",
      "100 12.726295232772827\n",
      "110 13.458921670913696\n",
      "120 14.38868522644043\n",
      "130 15.744547367095947\n",
      "140 16.639466524124146\n",
      "150 17.463393211364746\n",
      "160 18.32213807106018\n",
      "170 19.504474878311157\n",
      "180 20.501022338867188\n",
      "190 21.012498378753662\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "optimizer = optax.adamw(.001)\n",
    "state = create_train_state(init_rng, optimizer)\n",
    "del init_rng  # Must not be used anymore.\n",
    "model = resnet18()\n",
    "\n",
    "import time\n",
    "now = time.time()\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    data['image0'] = jnp.array(data['image0'].permute(0, 2, 3, 1))\n",
    "    data['label'] = jnp.array(data['label'])\n",
    "    state = training_step(state, data)\n",
    "    #logits = resnet18().apply({'params': state.params}, data['image0'], mutable=['batch_stats'])\n",
    "    if i % 10 == 0: print(i, time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0b38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from haiku_bn import BatchNorm\n",
    "\n",
    "\n",
    "class MLP(hk.Module):\n",
    "  \"\"\"One hidden layer perceptron, with normalization.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self._hidden_size = 10\n",
    "    self._output_size = 1\n",
    "    bn_config = dict()\n",
    "    bn_config.setdefault(\"create_scale\", True)\n",
    "    bn_config.setdefault(\"create_offset\", True)\n",
    "    bn_config.setdefault(\"decay_rate\", 0.999)\n",
    "    self._bn_config = bn_config\n",
    "\n",
    "  def __call__(self, inputs: jnp.ndarray, is_training: bool) -> jnp.ndarray:\n",
    "    out = hk.Linear(output_size=self._hidden_size, with_bias=True)(inputs)\n",
    "    out = BatchNorm(**self._bn_config)(out, is_training=is_training)\n",
    "    out = jax.nn.relu(out)\n",
    "    out = hk.Linear(output_size=self._output_size, with_bias=False)(out)\n",
    "    return out    \n",
    "\n",
    "def forward(x):\n",
    "  module = MLP()\n",
    "  return module(x, True)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "forward = hk.transform_with_state(forward)\n",
    "params, state = forward.init(init_rng, jnp.ones((4, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b726f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp/batch_norm/~/mean_ema': {'counter': DeviceArray(0., dtype=float32), 'hidden': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'average': DeviceArray([[ 0.34513226, -0.2222743 , -0.07778113, -0.05308276,\n",
      "               0.3091463 , -0.3052339 , -1.1058168 ,  0.82687795,\n",
      "               0.16870472, -0.6846972 ]], dtype=float32)}, 'mlp/batch_norm/~/var_ema': {'counter': DeviceArray(0., dtype=float32), 'hidden': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'average': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "febe9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.DeviceArray'> <class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "x = jax.random.normal(init_rng, (4, 3))\n",
    "y = jnp.array([1.0, 2.1, 1.4, 1.2])\n",
    "def loss_fn(state, params, x, y):\n",
    "    logits, state = forward.apply(params, state, None, x)\n",
    "    print(type(logits), type(y))\n",
    "    return ((logits - y)**2).mean()\n",
    "grad_fn = jax.grad(loss_fn)(state, params, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ec2b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlp/batch_norm/~/mean_ema': {'counter': DeviceArray(0, dtype=int32), 'hidden': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'average': DeviceArray([[ 0.34513226, -0.2222743 , -0.07778113, -0.05308276,\n",
      "               0.3091463 , -0.3052339 , -1.1058168 ,  0.82687795,\n",
      "               0.16870472, -0.6846972 ]], dtype=float32)}, 'mlp/batch_norm/~/var_ema': {'counter': DeviceArray(0, dtype=int32), 'hidden': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'average': DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fd1e4e97e47736202cb4acf08e256f8a33df263cdaff7616d1034cb9e837137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
